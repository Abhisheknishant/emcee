\documentclass[12pt,preprint]{aastex}

% has to be before amssymb it seems
\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}

\usepackage{url}
\usepackage{algorithmic,algorithm}
\usepackage{amssymb,amsmath}

\usepackage{listings}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\lstset{language=Python,
        basicstyle=\footnotesize\ttfamily,
        showspaces=false,
        showstringspaces=false,
        tabsize=2,
        breaklines=false,
        breakatwhitespace=true,
        identifierstyle=\ttfamily,
        keywordstyle=\bfseries\color[rgb]{0.133,0.545,0.133},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
        % numbers=left,
        % numberstyle=\tiny,
        % stepnumber=5,
        % numbersep=5pt,
    }

\newcommand{\project}[1]{{\sffamily #1}}
\newcommand{\Python}{\project{Python}}
\newcommand{\numpy}{\project{numpy}}
\newcommand{\Ubuntu}{\project{Ubuntu}}
\newcommand{\github}{\project{GitHub}}
\newcommand{\pip}{\project{pip}}
\newcommand{\acor}{\project{acor}}
\newcommand{\thisplain}{emcee}
\newcommand{\this}{\project{\thisplain}}
\newcommand{\paper}{document}
\newcommand{\license}{GNU General Public License v2}

% shortcuts for API layout -- this is some brittle stuff!
\newlength{\argmarg}
\setlength{\argmarg}{0.5in}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\class}[3]{\def\curclass{#1}
                \noindent{\scriptsize\this}\texttt{.}\code{#1{\small (#2)}}
                \begin{list}{}{\setlength{\leftmargin}{0.5\argmarg}}
                {\item #3}\end{list}}
\newcommand{\param}[3]{{\item {\code{#1}} {\footnotesize (\code{#2})}: #3}}
\newcommand{\exception}[2]{{\item \code{#1}: #2}}
\newcommand{\property}[3]{\param{#1}{#2}{#3}}
\newcommand{\method}[4]{{\item
    \noindent{\scriptsize\this\texttt{.}\code{\curclass}}%
    \texttt{.}{\bf \code{#1}}{\small\code{(#2)}}
    \begin{list}{}{\setlength{\leftmargin}{0.5\argmarg}}
    {\item #3}\end{list} \indent#4}}
\newenvironment{arglist}[1]{\noindent\hspace{0.5\argmarg}{\bf {#1}}
    \begin{list}{}{
        \setlength{\leftmargin}{\argmarg}
    }}
    {\end{list}}
\newenvironment{args}{\begin{arglist}{Arguments}}{\end{arglist}}
\newenvironment{kwargs}{\begin{arglist}{Keyword Arguments}}{\end{arglist}}
\newenvironment{errors}{\begin{arglist}{Exceptions}}{\end{arglist}}
\newenvironment{properties}{\begin{arglist}{Properties}}{\end{arglist}}
\newenvironment{yields}{\begin{arglist}{Yields}}{\end{arglist}}
\newenvironment{returns}{\begin{arglist}{Returns}}{\end{arglist}}
\newenvironment{methods}{\begin{arglist}{Methods}}{\end{arglist}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}

\newcommand{\Fig}[1]{Figure \ref{fig:#1}}
\newcommand{\fig}[1]{Figure \ref{fig:#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\Tab}[1]{Table \ref{tab:#1}}
\newcommand{\tab}[1]{Table \ref{tab:#1}}
\newcommand{\tablabel}[1]{\label{tab:#1}}
\newcommand{\Eq}[1]{Equation (\ref{eq:#1})}
\newcommand{\eq}[1]{Equation (\ref{eq:#1})}
\newcommand{\eqlabel}[1]{\label{eq:#1}}
\newcommand{\Sect}[1]{\S\ref{sect:#1}}
\newcommand{\sect}[1]{\S\ref{sect:#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}
\newcommand{\Algo}[1]{Algorithm \ref{algo:#1}}
\newcommand{\algo}[1]{Algorithm \ref{algo:#1}}
\newcommand{\algolabel}[1]{\label{algo:#1}}

% math symbols
\newcommand{\dd}{\mathrm{d}}
\newcommand{\like}{\mathscr{L}}
\newcommand{\bvec}[1]{\boldsymbol{#1}}
\newcommand{\paramvector}[1]{\bvec{#1}}
\newcommand{\normal}[2]{\mathcal{N} (#1, #2)}
\newcommand{\ensemble}{S}
\newcommand{\colorens}[1]{\ensemble^{(#1)}}
\newcommand{\red}{\colorens{0}}
\newcommand{\blue}{\colorens{1}}
\renewcommand{\vector}[1]{#1}
\renewcommand{\matrix}[1]{#1}
\newcommand{\pr}[1]{\ensuremath{p(#1)}}

% model parameters
\newcommand{\model}{\ensuremath{\vector{\Theta}}}
\newcommand{\data}{\ensuremath{\vector{D}}}
\newcommand{\nuisance}{\ensuremath{\vector{\alpha}}}
\newcommand{\link}{\ensuremath{X}}

% units
\newcommand{\unit}[1]{\mathrm{#1}}

% Citation alias
\defcitealias{Goodman:2010}{GW10}

\begin{document}

\title{\this: The MCMC Hammer}

\newcommand{\nyu}{2}
\newcommand{\mpia}{3}
\newcommand{\princeton}{4}
\newcommand{\courant}{5}
\author{Daniel~Foreman-Mackey\altaffilmark{1,\nyu},
    David~W.~Hogg\altaffilmark{\nyu,\mpia},
    Dustin~Lang\altaffilmark{\princeton},
    Jonathan~Goodman\altaffilmark{\courant}}
\altaffiltext{1}{To whom correspondence should be addressed:
                        \url{danfm@nyu.edu}}
\altaffiltext{\nyu}{Center for Cosmology and Particle Physics,
                        Department of Physics, New York University,
                        4 Washington Place, New York, NY, 10003, USA}
\altaffiltext{\mpia}{Max-Planck-Institut f\"ur Astronomie,
                        K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\altaffiltext{\princeton}{Princeton University Observatory,
                        Princeton, NJ, 08544, USA}
\altaffiltext{\courant}{Courant Institute, New York University,
                        251 Mercer St., New York, NY 10012, United States}

\begin{abstract}

    We introduce a stable, well tested Python implementation of the affine-%
    invariant ensemble sampler for Markov chain Monte Carlo (MCMC)
    proposed by Goodman \& Weare (2010). The code is open source and has
    already been used in several published projects in the Astrophysics
    literature. The algorithm behind \this\ has several advantages over
    traditional MCMC sampling methods and it has excellent performance as
    measured by the autocorrelation time.
    One major advantage of the algorithm is that it requires hand-tuning of
    only 2 parameters compared to $\sim N^2$ for
    a traditional algorithm in an $N$-dimensional parameter space. In this
    \paper, we describe the algorithm and the details of our implementation
    and API. Taking advantage of the naturally parallel nature of
    the algorithm, \this\ permits \emph{any} user to take advantage of
    multiple CPUs without extra effort. This is a huge advantage over a
    na\"ive implementation of the published algorithm when applied to
    computationally expensive problems.

    The code is available online at \url{http://danfm.ca/\thisplain} under the
    \license.

\end{abstract}

\keywords{
    methods: data analysis ---
    methods: numerical ---
    methods: statistical
}

\section{Introduction}

Probabilistic data analysis --- including Bayesian inference --- has
transformed scientific research in the past decade. Many of the most
significant gains have been due to numerical methods for approximate
inference; especially Markov chain Monte Carlo (MCMC). Many
problems in cosmology and astrophysics benefit from these advances because
the observations tend to have low signal-to-noise and be incomplete.

Probabilistic data analysis procedures involve examining and operating on
either the posterior probability density function (PDF) of the parameters
of the model or the likelihood function. In some cases it is
sufficient to just find the maximum of this function but it is often of
interest to (at least approximately) measure this distribution in more detail.
In particular, if we wish to propagate the effects of measurement
uncertainties through the analysis to the conclusions, we must sample the
PDF on sufficiently small scales. MCMC methods are designed to efficiently
sample and approximate the PDF even in high dimensional
parameter spaces. This has proved useful in too many research
applications to list here but the results from the Wilkinson Microwave
Anisotropy Probe (WMAP) mission provide a dramatic example
\citep[for example][]{Dunkley:2005}.

Arguably the most important advantage of Bayesian data analysis is
that it is possible to \emph{marginalize} over nuisance parameters. A
nuisance parameter is a component of the generative model that is of little
physical interest who's value must be known in order to produce data.
Marginalization is the process of integrating over all possible values of
these parameters and hence propagating the effects of your uncertainty about
the ``true'' value to the final result. The exact result of marginalization
is the marginalized likelihood function \pr{\data|\model}
of the set of observations \data given the physical model parameters
\model
\begin{equation}
    \eqlabel{marginalization}
    \pr{\data | \model} = \int
        \pr{\data | \model, \nuisance} \,
        \pr{\nuisance} \, \dd  \nuisance
\end{equation}
where \nuisance is the set of of nuisance parameters. In general, \data,
\model and \nuisance will all be lists of vectors. In some specific
cases, the integral in \eq{marginalization} is analytically tractable but in
general, the likelihood function
$p (\data | \model, \nuisance)$ is not a simple
integrable function. In fact, in many problems, the likelihood
function is actually the result of an extremely expensive numerical
simulation. In this regime, the integration must be calculated numerically
and it is often relatively efficient to approximate \eq{marginalization}
using MCMC sampling. If the likelihood function is expensive to
calculate, it is advantageous to use a sampling algorithm that reduces the
necessary number of likelihood evaluations. This also precludes the use of
second order methods (such as hybrid/Hamiltonian Monte Carlo) that require
the calculation of (numerical) gradients of the likelihood function.

Most uses of MCMC in the astrophysics literature are based on slight
modifications to the Metropolis-Hastings (M--H) method
\citep[for example][]{MacKay:2003}. Each step in a M--H chain is proposed
using a
multivariate Gaussian centered on the current position of the chain. Since
each term in the covariance matrix of this proposal distribution is an
unspecified parameter, this method has $N\,[N+1]/2$ tuning parameters (where
$N$ is the dimension of the parameter space).  To make matters worse, the
performance of this sampler is very sensitive to the optimality of these
tuning parameters and there is no fool-proof method for choosing the values
correctly. As a result, many heuristic methods have been developed to attempt
to determine the optimal parameters in a data-driven way
\citep[for example][]{Gregory:2005,Dunkley:2005,Widrow:2008}. Unfortunately,
these methods all require ``burn-in'' phases where shorter Markov chains
are sampled and the results are used to tune the hyperparameters. This extra
cost is unacceptable when the likelihood calls are computationally heavy.

The problem with traditional sampling methods can be visualized by looking
at the simple but highly anisotropic density
\begin{equation}
    \eqlabel{anisotropic}
    p(\mathbf{x}) \propto \exp \left (-\frac{(x_1-x_2)^2}{2\,\epsilon}
                                        - \frac{(x_1+x_2)^2}{2} \right )
\end{equation}
which would be considered ``difficult'' (in the low-$\epsilon$ regime) by
standard MCMC algorithms. In principle, it is possible to tune the
hyperparameters of a M--H sampler but if calculating the density in
\eq{anisotropic} is computationally expensive and the covariance is not
known \foreign{a priori} then the tuning procedure gets quickly intractable.
Also, since the number of parameters scales as $\sim N^2$, this problem gets
mush worse in higher dimensions.
\Eq{anisotropic} can, however, be transformed into the much easier problem of
sampling an isotropic Gaussian by an \emph{affine transformation} of the form
\begin{equation}
    y_1 = \frac{x_1-x_2}{\sqrt{\epsilon}} \, , \hspace{1cm} y_2 = x_1 + x_2.
\end{equation}
Therefore, an algorithm that is \emph{affine invariant} will be insensitive to
covariances between parameters. An affine invariant algorithm is unaffected by
any transformation of the density of the form
\begin{equation}
    \mathbf{Y} = \mathbf{A}\, \mathbf{X} + \mathbf{b}.
\end{equation}

Extending earlier work by \citet{Christen:2007},
\citet[][hereafter \citetalias{Goodman:2010}]{Goodman:2010} proposed an
affine invariant sampling
algorithm (\sect{algo}) with only two hyperparameters that can be tuned for
performance. \citet{Hou:2011} were the first group to implement this
algorithm to solve a physics problem. The implementation presented here is
an independent effort that has already proved effective in several projects
\citep[][Foreman-Mackey \& Widrow\ 2012, in prep.]{Lang:2011,
Bovy:2011, Dorman:2012}.
In what follows, we summarize the GW algorithm and the implementation
decisions made in \this. We also describe the small changes
that must be made to the algorithm to parallelize it. Finally, in \sect{api},
we outline the installation, usage and troubleshooting of the package.

\section{The Algorithm}\sectlabel{algo}

A complete discussion of MCMC methods is beyond the scope of this \paper.
Instead, the interested reader is directed to a classic reference like
\citet{MacKay:2003} and we will summarize some key concepts below.

The general goal of MCMC algorithms is to draw samples
$\{ \model_i \, \forall i=1, \ldots, M \}$ from
the joint probability distribution
\begin{equation}
    \pr{\model, \nuisance, \data} = \pr{\model, \nuisance}
            \, \pr (\data | \model, \nuisance)
\end{equation}
where the prior distribution $\pr{\model, \nuisance}$ and the likelihood
function $\pr{\data|\model,\nuisance}$ can be relatively easily (but not
necessarily quickly) computed for a particular value of
$(\model_i, \nuisance_i)$. Since the normalization $\pr{\data}$ is
independent of $\model$ and $\nuisance$, the joint distribution above is
proportional to the posterior probability $\pr{\model, \nuisance | \data}$
given any one choice of generative model. Therefore, once the samples
produced by MCMC are available, the marginalized constraints on $\model$
(\eq{marginalization}) can be approximated by the histogram of the samples
projected into the subspace spanned by $\model$. In particular, the
expectation value of a particular parameter $\phi \in \model$ given the
samples $\{ \phi_i \}$ is
\begin{equation}
    E[\phi] = \int \phi \, \pr{\model, \nuisance | \data}
            \, \dd \model \, \dd \nuisance
            \approx \frac{1}{M} \sum_{i=1} ^M \phi_i .
\end{equation}
Generating the samples $\model_i$ is a non-trivial process unless
$\pr{\model, \nuisance, \data}$ is a very specific analytic distribution
(for example, a Gaussian). MCMC is a procedure for generating a random walk
in the parameter space the relatively efficiently draws a representative set
of samples from the distribution. Each point in a Markov chain
$\link (t) = [\model(t), \nuisance(t)]$
depends only on the position of the previous link $\link (t-1)$.

\paragraph{The Metropolis-Hastings (M--H) Algorithm}

The simplest and most commonly used MCMC algorithm is the M--H method
(\algo{mh}). The iterative procedure is as follows: (1) given a position
$X(t)$ sample a proposal position $Y$ from the transition distribution
$Q(Y; X(t))$, (2) accept this proposal with probability
\begin{equation}
    \mathrm{min} \left \{ 1,
            \frac{\pr{\vector{Y} | \data}}{\pr{\vector{X}(t) | \data}} \,
            \frac{Q(X(t); Y)}{ Q(Y;X(t))}  \right \}.
\end{equation}
It is worth emphasizing that if this step is accepted $X(t+1) = Y$; Otherwise,
the new position is set to the previous one $X(t+1) \gets X(t)$ (in other
words, the position $X(t)$ is \emph{double counted}).

The M--H algorithm converges (as $t \to \infty$) to a stationary set of
samples from the distribution but there are many algorithms with faster
convergence and varying levels of implementation difficulty (CITECITECITE).
Faster convergence is preferred because of the reduction of computational
cost due to the smaller number of likelihood computations necessary to obtain
the equivalent level of accuracy. The efficiency of an algorithm can be
measured by the autocorrelation function and more specifically, the integrated
autocorrelation time (see \sect{tests}). This quantity is an estimate of the
number of steps needed in the chain in order to draw independent samples from
the target density. Therefore, a more efficient chain will have a shorter
autocorrelation time.

\begin{algorithm}
\caption{The procedure for a single Metropolis-Hastings MCMC step.
    \algolabel{mh}}
\begin{algorithmic}[1]

\STATE Draw a sample $Y \sim Q (Y; X(t))$
\STATE $q \gets [\pr{\vector{Y}} \, Q(X(t); Y)]
        / [\pr{\vector{X}(t)} \, Q(Y;X(t))]$
\STATE $r \gets R \sim [0, 1]$
\IF{$r \ge q$}
    \STATE $\vector{X}(t+1) \gets \vector{Y}$
\ELSE
    \STATE $\vector{X}(t+1) \gets \vector{X}(t)$
\ENDIF

\end{algorithmic}
\end{algorithm}

\paragraph{The stretch move}

\citetalias{Goodman:2010} proposed an affine invariant ensemble sampling
algorithm informally called the ``stretch move''. For completeness and for
clarity of notation, we summarize the algorithm here and refer the interested
reader to the original paper for more details. This method involves
simultaneously evolving an ensemble of $K$ \emph{walkers}
$\ensemble = \{ \vector{X_j}, \forall j=1,\ldots,K \}$ where the proposal
distribution for one walker $k$ is based on the current positions of the
$K-1$ walkers in the \emph{complementary ensemble}
$\ensemble_{[k]} = \{ \vector{X_j}, \forall j \ne k \}$. In general, each
$\vector{X_j}$ is also a vector in $N$ dimensions (the dimension
of the parameter space).

To update the position of a walker at position $\vector{X_k}$,
another walker $\vector{X_j}$ with $j \ne k$ is randomly chosen and then
a new position is proposed:
\begin{equation}
    \eqlabel{proposal}
    \vector{X_k} (t) \to \vector{Y} = \vector{X_j}
            + Z \, [\vector{X_k} (t) - \vector{X_j}]
\end{equation}
where $Z$ is a random variable drawn from a distribution $g(Z = z)$.
It is clear that if $g$ satisfies
\begin{equation}
    g(z^{-1}) = z \, g(z),
\end{equation}
the proposal of \eq{proposal} is symmetric. In this case, the chain will
satisfy detailed balance if the proposal is accepted with probability
\begin{equation}
    \eqlabel{acceptance}
    q = \min \left \{ 1, Z^{n-1} \,
                \frac{\pr{\vector{Y}}}{\pr{\vector{X_k} (t)}} \right \}
\end{equation}
where $n$ is the dimension of the parameter space. This procedure is then
repeated for each walker in the ensemble \emph{in series} following the
procedure shown in \algo{goodman}.

\citetalias{Goodman:2010} advocate for a particular form of $g(z)$, namely
\begin{equation}
    \eqlabel{goodmanprop}
    g(z) \propto \left \{ \begin{array}{ll}
        \displaystyle\frac{1}{\sqrt{z}} & \mathrm{if}\, z\in
                        \left [ \displaystyle\frac{1}{a}, a \right ], \\
        0 & \mathrm{otherwise}
    \end{array} \right .
\end{equation}
where $a$ is an adjustable scale parameter that \citetalias{Goodman:2010} set
to 2.

\begin{algorithm}
\caption{A single stretch move update step from \citetalias{Goodman:2010} where
    line \ref{line:hard} is generally the most computationally expensive
    step. \algolabel{goodman}}
\begin{algorithmic}[1]
\FOR{$k = 1, \ldots, K$}
    \STATE Draw a walker $X_j$ at random from the complementary ensemble %
        $\ensemble_{[k]}(t)$
    \STATE $z \gets Z \sim g(z)$, \Eq{goodmanprop}
    \STATE $\vector{Y} \gets \vector{X_j} %
                + z \, [ \vector{X_k} (t) - \vector{X_j}]$
    \STATE $q \gets z^{n-1} \, p(Y)/p(X_k(t))$ \label{line:hard}
    \STATE $r \gets R \sim [0, 1]$
    \IF{$R \ge q$, \eq{acceptance}}
        \STATE $X_k(t+1) \gets Y$
    \ELSE
        \STATE $X_k(t+1) \gets X_k(t)$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\paragraph{The parallel stretch move}

It is tempting to na\"ively parallelize the stretch move algorithm by
simultaneously advancing each walker based on the state of the ensemble
instead of evolving the walkers in series. Unfortunately, this would no
longer satisfy detailed balance. Instead, we must split the full ensemble
into two subsets
($\red = \{ \vector{X_k} \, \forall k = 1, \ldots, K/2 \}$ and
$\blue = \{ \vector{X_k} \, \forall k = K/2+1, \ldots, K \}$) and
simultaneously update all the walkers in $\red$
--- using the stretch move procedure from \algo{goodman} ---
based \emph{only} on the positions of the walkers in the other set
($\blue$). Then, using the new positions $\red$,
we can update $\blue$. In this case, the outcome is a valid step
for all of the walkers. The pseudocode for
this procedure is shown in \algo{parallel}. This code appears similar to
\algo{goodman} but now the computationally expensive inner loop
(starting at line \ref{line:parallelloop} in \algo{parallel}) can be run in
parallel.

The performance of this method --- quantified by the autocorrelation time ---
is comparable to the traditional stretch move algorithm but the fact that one
can now take advantage of generic parallelization makes this generalization
extremely powerful.

\begin{algorithm}
\caption{The parallel stretch move update step
    \algolabel{parallel}}
\begin{algorithmic}[1]
\FOR{$i \in \{0, 1\}$}
    \FOR{$k = 1, \ldots, K/2$} \label{line:parallelloop}
        \STATE Draw a walker $\vector{X_j}$ at random from the complementary %
            ensemble $\colorens{\sim i} (t)$
        \STATE $\vector{X_k} \gets \colorens{i}_k$
        \STATE $z \gets Z \sim g(z)$, \Eq{goodmanprop}
        \STATE $\vector{Y} \gets \vector{X_j}
                + z \, [ \vector{X_k} (t) - \vector{X_j}]$
        \STATE $q \gets z^{n-1} \, p(\vector{Y})/p(\vector{X}_k(t))$
        \STATE $r \gets R \sim [0, 1]$
        \IF{$r \ge q$, \eq{acceptance}}
            \STATE $\vector{X_k} (t+\frac{1}{2}) \gets \vector{Y}$
        \ELSE
            \STATE $\vector{X_k} (t+\frac{1}{2}) \gets \vector{X_k}(t)$
        \ENDIF
    \ENDFOR
    \STATE $t \gets t+\frac{1}{2}$
\ENDFOR

\end{algorithmic}
\end{algorithm}

\section{Benchmarks \& Tests} \sectlabel{tests}

\paragraph{Measuring the performance}

A standard method of quantifying the performance of an MCMC sampler is to
estimate the autocorrelation time of the sampler on several densities.

The main goal of running a Markov chain is to measure the expectation value
(and variance) of a particular value (e.g.~$f$)
\begin{equation}
    \left < f(\mathbf{x}) \right > = \int f(\mathbf{x}) \, p (\mathbf{x}) \,
            \dd \mathbf{x}
\end{equation}
which can be approximated as
\begin{equation}
    \eqlabel{schainestim}
    \left < f(\mathbf{x}) \right > \approx \frac{1}{T_s} \sum_{t=1}^{T_s}
            f(\mathbf{X}(t))
\end{equation}
where $T$ is the length of the chain.  The generalization of \eq{schainestim}
to the case of the ensemble sampler is
\begin{equation}
    \eqlabel{echainestim}
    \left < f(\mathbf{x}) \right > \approx \frac{1}{T_e} \sum_{t=1}^{T_s}
        \left [ \frac{1}{K} \sum_{k = 1}^{K} f(\mathbf{X}_k(t)) \right ]
\end{equation}
where $K$ is the number of walkers.  The autocorrelation function of the chain
is then given by
\begin{equation}
    C (t) = \frac{1}{K^2} \lim_{t^\prime \to \infty} \mathrm{cov}
            \left [ \sum_{k = 1}^{K} f(\mathbf{X}_k (t+t^\prime)),
            \sum_{k = 1}^{K} f(\mathbf{X}_k (t^\prime)) \right ]
\end{equation}
and the integrated autocorrelation time is given by
\begin{equation}
    \tau = \sum_{t= -\infty} ^{\infty} \frac{C(t)}{C(0)} .
\end{equation}

\this\ can optionally calculate the autocorrelation time using the Python
module \project{acor}\footnote{\url{http://github.com/dfm/acor}} to estimate
the autocorrelation time. This module is a direct port of the original
algorithm \citepalias[described by][]{Goodman:2010} and implemented by those
authors in
C++.\footnote{\url{http://www.math.nyu.edu/faculty/goodman/software/acor}}

\paragraph{Multivariate Gaussian distribution}

The simplest test of an MCMC sampler is its sampling performance on a highly
covariant multivariate Gaussian density. For the tests in the paper, we
randomly generated a 50 dimensional positive definite covariance tensor and
initial conditions for each walker. Then, each sampler was tested with the
same initial conditions for its performance on the density
\begin{equation}
    \pi (\mathbf{x}) \propto \exp\left ( -\frac{1}{2} \mathbf{x}^T \,
                                \Sigma^{-1} \, \mathbf{x} \right )
\end{equation}
for the same covariance tensor $\Sigma$ in each trial.

DFM: actually do some tests here...

\paragraph{Rosenbrock density}

\section{Discussion \& Tips}

% DFM: In this section, I put the key advice from each paragraph in
% \emph{}.  Hogg

The goal of this project has been to make a sampler that is a useful
tool for a large class of data analysis problems---a ``hammer'' if you
will.  If development of statistical and data-analysis understanding
is the key goal, a user who is new to MCMC benefits enormously by
writing her or his own Metropolis--Hastings code (\algo{mh}) from
scratch before downloading \this.  For typical problems, the
\this\ package will perform better than any home-built M--H code (for
all the reasons given above), but the intuitions developed by writing
and tuning a self-built MCMC code cannot be replaced by reading this
document and running this pre-built package.  That said, once those
intuitions are developed, it makes sense to switch to \this\ or a
similarly well engineered piece of code for performance on large
problems.

Ensemble samplers like \this\ require some thought for
initialization.  One general approach is to start the walkers at a
sampling of the prior or spread out over a reasonable range in
parameter space.  Another general approach is to start the walkers in
a very tight $N$-dimensional ball in parameter space around one point
that is expected to be close to the maximum probability point.  The
first is more objective but, in practice, we find that the latter is
much more effective if there is any chance of walkers getting stuck in
low probability modes of a multi-modal probability landscape.  A third
approach would be to start from a sampling of the prior, and go
through a ``burn-in'' phase in which the prior is transformed
continuously into the posterior by increasing the ``temperature''.
Discussion of this kind of annealing is beyond the scope of this
document.

The acceptance ratio (DFM: Is there a function to display this and
have you defined this concept above?  If not, please do.)  should be
in the 0.2 to 0.5 range (there are theorems about this for specific
problems; RTFL).  In principle, if the acceptance ratio is too low,
you can raise it by decreasing the $a$ parameter; and if it is too
high, you can reduce it by increasing the $a$ parameter.  However, in
practice, we find that $a=2$ is good in essentially all situations.
That means that \emph{if the acceptance ratio is getting very low,
  something is going very wrong}.  Typically a low acceptance ratio
means that the posterior probability is multi-modal, with the modes
separated by wide, low probability ``valleys''.  In situations like
these, the best idea (though expensive of human time) is to split the
space into disjoint single-mode regions and sample each one
independently, combining the independently sampled regions
``properly'' (also expensive, and beyond the scope of this document)
at the end.  In previous work, we have advocated clustering methods to
remove multiple modes \citep{Hou:2011} which work well, but only when
the different modes have \emph{very} different posterior
probabilities.

In general, you want to \emph{run with large numbers of walkers}, like
hundreds.  In principle, there is no reason not to go large when it
comes to walker number, until you hit performance issues.  Although
each step takes twice as much compute time if you double the number of
walkers, it also returns to you twice as many independent samples per
autocorrelation time.  So go large.  DFM: A list of problems solved by
increasing the number of walkers.

One mistake many users of MCMC methods make is to take \emph{too many}
samples!  If all you want your MCMC to do is produce one- or
two-dimensional error bars on two or three parameters, then you only
need dozens of independent samples.  With the ensemble sampling, you
get this from a \emph{single snapshot} or single timestep, provided
that you are using dozens of walkers (and we would recommend that you
use hundreds in most applications).  The key point is that \emph{you
  should run the sampler for a few (say 10) autocorrelation times.}
Once you have run that long, no matter how you initialized the
walkers, the set of walkers you obtain at the end should be an
independent set of samples from the distribution, of which you rarely
need many.

Another common mistake, of course, is to run the sampler for \emph{too
  few} steps.  You can identify that you haven't run for enough steps
in a couple of ways: If you plot the parameter values in the ensemble
as a function of step number, you will see large-scale variations over
the full run length if you have gone less than an autocorrelation
time.  You will also see that if you try to measure the
autocorrelation time (with, say, \acor), it will give you a time that
is always a significant fraction of your run time; it is only when the
correlation time is much shorter (say by a factor of 10) than your run
time that you are sure to have run long enough.  The danger of both of
these methods---an unavoidable danger at present---is that you can
have a huge dynamic range in contributions to the autocorrelation
time; you might think it is 30 when in fact it is 30\,000, but you
don't ``see'' the 30\,000 in a run that is only 300 steps long.  There
is not much you can do about this; it is generic when the posterior is
multimodal: The autocorrelation time within each mode can be short but
the mode--mode migration time can be long.  See above on low
acceptance ratio; in general when your acceptance ratio gets low your
autocorrelation time is very, very long.

One significant limitation to the stretch move and moves like it is
that they implicitly assumes that the parameters can be assembled into
a vector-like object on which linear operations can be performed.
This is not (trivially) true for parameters that have non-trivial
constraints, like parameters that must be integer-valued or
equivalent, or parameters that are subject to deterministic non-linear
constraints.  Sometimes these issues can be avoided by
reparameterization, but in some cases, samplers like \this\ will not
be useful, or might require clever or interesting improvements.  The
\this\ package is open-source software; please push us changes!

\acknowledgments It is a pleasure to thank Jo Bovy (IAS) and Larry
Widrow (Queens) for many helpful contributions to the ideas and code
presented here.  This work makes use of the open-source Python
\numpy\ package.

\begin{thebibliography}{}
\raggedright

\bibitem[Bovy\ \etal(2011)]{Bovy:2011}
 Bovy,~J., Rix,~H.-W., Liu,~C., Hogg,~D.~W., Beers,~T.~C., \& Lee,
Y.~S., 2011, \apj, submitted, arXiv:1111.1724 [astro-ph.GA]
% http://adsabs.harvard.edu/cgi-bin/bib_query?arXiv:1111.1724

\bibitem[Christen(2007)]{Christen:2007}
{Christen}, J., \emph{A general purpose scale-independent MCMC algorithm},
technical report I-07-16, CIMAT, Guanajuato, 2007.

\bibitem[Dorman\ \etal(2012)]{Dorman:2012}
{Dorman},~C., {Guhathakurta},~P., {Fardal},~M.~A., {Geha},~M.~C.,
{Howley},~K.~M., {Kalirai},~J.~S., {Lang},~D., {Cuillandre}, J.,
{Dalcanton},~J., {Gilbert},~K.~M., {Seth},~A.~C., {Williams},~B.~F.,
\& {Yniguez},\ B., 2012, \apj, submitted
% http://adsabs.harvard.edu/abs/2012AAS...21934608D

\bibitem[Dunkley\ \etal(2005)]{Dunkley:2005}
{Dunkley}, J., {Bucher}, M., {Ferreira}, P.~G., {Moodley}, K.,
\& {Skordis}, C.,
2005, \mnras, 356, 925-936
% http://adsabs.harvard.edu/abs/2005MNRAS.356..925D

\bibitem[Goodman~\&\ Weare(2010)]{Goodman:2010}
Goodman,~J., \& Weare,\ J.,
2010, Comm.\ App.\ Math.\ Comp.\ Sci., 5, 65

\bibitem[Gregory(2005))]{Gregory:2005}
{Gregory}, P.~C.,
\emph{Bayesian Logical Data Analysis for the Physical Sciences},
Cambridge University Press, 2005
% http://adsabs.harvard.edu/abs/2005blda.book.....G

\bibitem[Hou\ \etal(2011))]{Hou:2011}
{Hou}, F., {Goodman}, J., {Hogg}, D.~W., {Weare}, J., \& {Schwab}, C.,
2011, arXiv:1104.2612
% http://adsabs.harvard.edu/abs/2011arXiv1104.2612H

\bibitem[Lang\ \& Hogg(2011))]{Lang:2011}
{Lang}, D. and {Hogg}, D.~W.,
2011, arXiv:1103.6038
% http://adsabs.harvard.edu/abs/2011arXiv1103.6038L

\bibitem[MacKay(2003))]{MacKay:2003}
{MacKay}, D., \emph{Information Theory, Inference, and Learning Algorithms},
Cambridge University Press, 2003

\bibitem[Widrow\ \etal(2008))]{Widrow:2008}
{Widrow}, L.~M. and {Pym}, B. and {Dubinski}, J.,
2008, \apj, 679, 1239
% http://adsabs.harvard.edu/abs/2008ApJ...679.1239W

\end{thebibliography}

\appendix

\section{Usage}\sectlabel{api}

\paragraph{Installation}

The easiest way to install \this\ is using
\pip\footnote{\url{http://pypi.python.org/pypi/pip/}}. Running the command
\begin{lstlisting}
% pip install emcee
\end{lstlisting}
at the command line of a UNIX-based system will install the package and its
\Python\ dependencies. If you would like to install for all users, you might
need to run the above command with superuser permissions. \this\ depends on
\Python\ ($>2.7$) and \numpy\footnote{\url{http://numpy.scipy.org}} ($>1.6$)
and the associated \texttt{dev} headers. On some systems, you might need to
install these packages separately. On most systems where \Python\ has already
been installed, this won't be necessary but if it is, you can install
dependencies (on \Ubuntu, for example) with the command:
\begin{lstlisting}
% apt-get python python-dev numpy numpy-dev
\end{lstlisting}

An alternative installation method is to download the source code from
\url{http://danfm.ca/emcee} and run
\begin{lstlisting}
% python setup.py install
\end{lstlisting}
in the unzipped directory. Make sure that you have \numpy\ installed in this
case as well.

\paragraph{Issues \& Contributions}

The development of \this\ is being coordinated on \github\ at
\url{http://github.com/dfm/emcee} and contributions are welcome. If you
encounter any problems with the code, please report them at
\url{http://github.com/dfm/emcee/issues} (DFM: check this url) and consider
contributing a patch.

\paragraph{Basic usage}

A very simple sample problem and a common unit test for sampling code is
the performance of the code on a high dimensional Gaussian density
\begin{equation}\eqlabel{mgauss}
    \pr{\mathbf{x}} \propto \exp\left ( -\frac{1}{2} \mathbf{x}^T \,
                                \Sigma^{-1} \, \mathbf{x} \right ).
\end{equation}
We will incrementally work through this example here and the source code
is available online.%
\footnote{\url{https://github.com/dfm/emcee/blob/master/quickstart.py}}

First, import the necessary packages and define the function that calculates
the logarithm of \eq{mgauss}.
\begin{lstlisting}
import numpy as np
import emcee

def lnprobfn(x, mu, icov):
    diff = x-mu
    return -np.dot(diff,np.dot(icov,diff))/2.0
\end{lstlisting}
Next, choose the number of dimensions to use and generate the mean vector
\code{means} and the positive definite covariance matrix \code{cov} and its
inverse \code{icov}.
\begin{lstlisting}
ndim  = 10
means = np.random.rand(ndim)
cov   = 0.5-np.random.rand(ndim**2).reshape((ndim, ndim))
cov   = np.triu(cov)
cov  += cov.T - np.diag(cov.diagonal())
cov   = np.dot(cov,cov)
icov  = np.linalg.inv(cov)
\end{lstlisting}
The number of walkers to use is one of the tuning parameters of this sampler
and we'll come back to some discussion of how to choose this shortly. For
now, use 100 walkers and generate an initial guess for the position of each
of the walkers (in a small ball centered at zero mean).
\begin{lstlisting}
nwalkers = 100
p0 = [np.random.rand(ndim) for i in xrange(nwalkers)]
\end{lstlisting}
Initialize the sampler object passing the chosen hyperparameters and the
generated likelihood function and it's arguments.
\begin{lstlisting}
sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprobfn,
                        args=[means, icov])
\end{lstlisting}
Now, run a ``burn-in'' chain of 500 steps. Note that each walker is advanced
500 times here so \code{lnprobfn} is actually be called $5\times10^4$ times.
\begin{lstlisting}
pos, prob, state = sampler.run_mcmc(p0, 500)
\end{lstlisting}
Finally, reset the chain and sample 2000 steps starting from the final state
of the burn-in chain.
\begin{lstlisting}
sampler.reset()
sampler.run_mcmc(pos, 2000, rstate0=state)
\end{lstlisting}

\section{API}

\paragraph{\code{\thisplain.ensemble}}

\class{EnsembleSampler}{*args, **kwargs}
    {Ensemble sampling following Goodman \& Weare (2010) with optional
    parallelization}

\begin{args}
    \param{k}{int}{The number of Goodman \& Weare ``walkers''.}
    \param{dim}{int}{Number of dimensions in the parameter space.}
    \param{lnpostfn}{callable}{%
        A function that takes a vector in the parameter space as input and
        returns the natural logarithm of the posterior probability for that
        position.
    }
\end{args}

\begin{kwargs}
    \param{a}{float}{The proposal scale parameter from \eq{goodmanprop}.
                     (default: \code{2.0})}
    \param{args}{list}{%
        Optional list of extra arguments for \code{lnpostfn}. \code{lnpostfn}
        will be called with the sequence \code{lnpostfn(p, *args)}.
    }
    \param{postargs}{list}{Alias of \code{args} for backwards compatibility.}
    \param{threads}{int}{The number of threads to use for parallelization.
        If \code{threads == 1}, then the \code{multiprocessing} is not used
        but if \code{threads > 1}, then a \code{Pool} object
        is created and calls to \code{lnpostfn} are run in parallel following
        \algo{parallel}. (default: \code{1})}
    \param{pool}{multiprocessing.Pool}{An alternative method of using the
        parallelized algorithm. If \code{pool is not None}, the value of
        \code{threads} is ignored and the provided \code{Pool} is used for
        all parallelization. (default: \code{None})}
\end{kwargs}

\begin{errors}
    \exception{AssertionError}{If \code{k < 2*dim} or if \code{k} is not even.}
\end{errors}

\begin{properties}
    \property{chain}{numpy.ndarray}{A pointer to the Markov chain itself. The
        shape of this array is \code{(k, dim, iterations/resample)}}
    \property{flatchain}{numpy.ndarray}{A shortcut for accessing \code{chain}
        flattened along the zeroth (walker) axis.}
    \property{lnprobability}{numpy.ndarray}{A pointer to the matrix of the
        value of \code{lnprobfn} produced at each step for each walker. The
        shape is \code{(k, iterations/resample)}.}
    \property{iterations}{int}{The number of steps that have been run in the
        chain.}
    \property{acceptance\_fraction}{numpy.ndarray}{An array (length: \code{k})
        of the fraction of steps accepted for each walker.}
    \property{acor}{numpy.ndarray}{The autocorrelation time of each parameter
        in the chain (length: \code{dim}) as estimated by the \code{acor}
        module.}
    \property{random\_state}{tuple}{The state of the internal random number
        generator. In practice, it's the result of calling \code{get\_state()}
        on a \code{numpy.random.mtrand.RandomState} object. You can try to
        set this property but be warned that if you do this and it fails, it
        will do so silently.}
\end{properties}

\begin{methods}
    \method{sample}{pos0, lnprob0=None, rstate0=None, iterations=1}
        {Advances the chain \code{iterations} steps as an iterator}{%
            \begin{args}
                \param{pos0}{numpy.ndarray}{
                    A list of the initial positions of the walkers in the
                    parameter space. The shape is \code{(k, dim)}.}
            \end{args}
            \begin{kwargs}
                \param{lnprob0}{numpy.ndarray}{
                    The list of log posterior probabilities for the walkers
                    at positions given by the \code{p0}. If \code{lnprob is
                    None}, the initial values are calculated. The shape is
                    \code{(k, dim)}.}
                \param{rstate0}{tuple}{The state of the random number
                    generator. \\
                    See \code{EnsembleSampler.random\_state} for details.}
                \param{iterations}{int}{The number of steps to run.
                    (default: \code{1})}
            \end{kwargs}
            \begin{yields}
                \param{pos}{numpy.ndarray}{
                    A list of the current positions of the walkers in the
                    parameter space. The shape is \code{(k, dim)}.}
                \param{lnprob}{numpy.ndarray}{
                    The list of log posterior probabilities for the walkers
                    at positions given by the \code{pos}. The shape is
                    \code{(k, dim)}.}
                \param{rstate}{tuple}{The state of the random number
                    generator.}
            \end{yields}
        }
    \method{run\_mcmc}{pos0, N, rstate0=None, lnprob0=None, **kwargs}
        {Iterate \code{sample} for \code{N} iterations and return the result.
        The parameters are passed directly to \code{sample} so see above for
        details.}{%
            \begin{returns}
                \param{pos}{numpy.ndarray}{
                    A list of the final positions of the walkers in the
                    parameter space. The shape is \code{(k, dim)}.}
                \param{lnprob}{numpy.ndarray}{
                    The list of log posterior probabilities for the walkers
                    at positions given by the \code{pos}. The shape is
                    \code{(k, dim)}.}
                \param{rstate}{tuple}{The final state of the random number
                    generator.}
            \end{returns}
        }
    \method{reset}{}{Clear \code{chain}, \code{lnprobability} and the
        bookkeeping parameters.}{}
    \method{clear\_chain}{}{An alias of \code{reset} kept for backwards
        compatibility.}{}

\end{methods}


\end{document}


