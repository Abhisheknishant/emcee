\documentclass[12pt,preprint]{aastex}

% has to be before amssymb it seems
\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}

\usepackage{url}
\usepackage{algorithmic,algorithm}
\usepackage{amssymb,amsmath}

\usepackage{listings}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\lstset{language=Python,
        basicstyle=\footnotesize\sffamily,
        showspaces=false,
        showstringspaces=false,
        tabsize=2,
        breaklines=false,
        breakatwhitespace=true,
        identifierstyle=\ttfamily,
        keywordstyle=\bfseries\color[rgb]{0.133,0.545,0.133},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
        }

\newcommand{\project}[1]{{\sffamily #1}}
\newcommand{\Python}{\project{Python}}
\newcommand{\numpy}{\project{numpy}}
\newcommand{\Ubuntu}{\project{Ubuntu}}
\newcommand{\github}{\project{GitHub}}
\newcommand{\pip}{\project{pip}}
\newcommand{\acor}{\project{acor}}
\newcommand{\thisplain}{emcee}
\newcommand{\this}{\project{\thisplain}}
\newcommand{\paper}{\emph{Article}}
\newcommand{\license}{GNU Public License (v2)}

% shortcuts for API layout -- this is some brittle stuff!
\newlength{\argmarg}
\setlength{\argmarg}{0.5in}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\class}[3]{\def\curclass{#1}
                \noindent{\scriptsize\this}\texttt{.}\code{#1{\small (#2)}}
                \begin{list}{}{\setlength{\leftmargin}{0.5\argmarg}}
                {\item #3}\end{list}}
\newcommand{\param}[3]{{\item {\code{#1}} {\footnotesize (\code{#2})}: #3}}
\newcommand{\exception}[2]{{\item \code{#1}: #2}}
\newcommand{\property}[3]{\param{#1}{#2}{#3}}
\newcommand{\method}[4]{{\item
    \noindent{\scriptsize\this\texttt{.}\code{\curclass}}\texttt{.}{\bf \code{#1}}%
    {\small\code{(#2)}}
    \begin{list}{}{\setlength{\leftmargin}{0.5\argmarg}}
    {\item #3}\end{list} \indent#4}}
\newenvironment{arglist}[1]{\noindent\hspace{0.5\argmarg}{\bf {#1}}
    \begin{list}{}{
        \setlength{\leftmargin}{\argmarg}
    }}
    {\end{list}}
\newenvironment{args}{\begin{arglist}{Arguments}}{\end{arglist}}
\newenvironment{kwargs}{\begin{arglist}{Keyword Arguments}}{\end{arglist}}
\newenvironment{errors}{\begin{arglist}{Exceptions}}{\end{arglist}}
\newenvironment{properties}{\begin{arglist}{Properties}}{\end{arglist}}
\newenvironment{yields}{\begin{arglist}{Yields}}{\end{arglist}}
\newenvironment{returns}{\begin{arglist}{Returns}}{\end{arglist}}
\newenvironment{methods}{\begin{arglist}{Methods}}{\end{arglist}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}

\newcommand{\Fig}[1]{Figure \ref{fig:#1}}
\newcommand{\fig}[1]{Figure \ref{fig:#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\Tab}[1]{Table \ref{tab:#1}}
\newcommand{\tab}[1]{Table \ref{tab:#1}}
\newcommand{\tablabel}[1]{\label{tab:#1}}
\newcommand{\Eq}[1]{Equation (\ref{eq:#1})}
\newcommand{\eq}[1]{Equation (\ref{eq:#1})}
\newcommand{\eqlabel}[1]{\label{eq:#1}}
\newcommand{\Sect}[1]{\S (\ref{sect:#1})}
\newcommand{\sect}[1]{\S (\ref{sect:#1})}
\newcommand{\sectlabel}[1]{\label{sect:#1}}
\newcommand{\Algo}[1]{Algorithm (\ref{algo:#1})}
\newcommand{\algo}[1]{Algorithm (\ref{algo:#1})}
\newcommand{\algolabel}[1]{\label{algo:#1}}

% math symbols
\newcommand{\dd}{\mathrm{d}}
\newcommand{\like}{\mathscr{L}}
\newcommand{\bvec}[1]{\boldsymbol{#1}}
\newcommand{\normal}[3]{\mathcal{N} (#1 | #2, #3)}

% model parameters
\newcommand{\model}{\paramvector{\Theta}}
\newcommand{\data}{\paramvector{X}}

% units
\newcommand{\unit}[1]{\mathrm{#1}}

\begin{document}

\title{\this: Ensemble MCMC for the masses}
\author{Daniel~Foreman-Mackey\altaffilmark{1}, David~W.~Hogg, \etal}
\affil{Center for Cosmology and Particle Physics,
                         Department of Physics, New York University,
                         4 Washington Place, New York, NY, 10003, USA}
\altaffiltext{1}{danfm@nyu.edu}

\begin{abstract}

    \this~is a stable, well tested Python implementation of the affine-%
    invariant ensemble sampler for Markov chain Monte Carlo (MCMC)
    proposed by Goodman~\&~Weare (2010). The code is open source and has
    already been used in several published projects in the Astrophysics
    literature. The algorithm behind \this~has several advantages over
    traditional MCMC sampling methods and it has excellent performance
    measured by the sampling autocorrelation time on several test problems.
    One major advantage of the algorithm is that it requires the hand-%
    tuning of only 2 parameters compared to the $\sim N^2$ for
    a traditional algorithm in $N$ dimensions. In this \paper, we describe
    the Goodman~\&~Weare algorithm and the details of our implementation.
    \this~takes advantage of the naturally parallel nature of the algorithm
    allowing \emph{any} user to take advantage of multiple CPUs without any
    extra effort. We discuss the subtleties associated with this
    parallelization.

    The code is available online at \url{http://danfm.ca/\thisplain} under the
    \license.

\end{abstract}

\keywords{
    methods: data analysis ---
    methods: numerical ---
    methods: statistical
}

\section{Introduction}

Probabilistic data analysis is a common theme in the recent scientific
literature since. In particular, many problems in astrophysics and cosmology,
benefit from careful probabilistic inference because most problems of
interest lie in the regime of very low signal-to-noise date with large
systematic uncertainties and missing observations. In many scientific
problems of interest, however, there is a physically motivated,
generative model of the data or several
such models that we wish to compare. If we can leverage the physics that we
understand and the rich prior information from the literature then there is
a chance --- it has been repeatedly shown --- of solving these problems.
With the use of more sophisticated models for the data comes an engineering
challenge that can only be solved with a combination of computing power and
more efficient algorithms.

The general probabilistic data analysis procedure involves examining either
the posterior probability distribution function (PDF) of the parameters
of the model or the likelihood function of the data. In some cases, it is
sufficient to just find the maximum of this function but it is often of
interest to (at least approximately) understand the details of the function.
In particular, if we wish to propagate the effects of measurement
uncertainties through to the result of the analysis, we must sample the
PDF on sufficiently small scales. Markov chain Monte Carlo (MCMC)
methods are designed to approximate this function even in high dimensional
parameter spaces and the use of MCMC has proved useful in too many research
applications to list here (e.g. WMAP, etc. DFM: add citations).

Arguably the most important advantage of fully probabilistic data analysis is
that it is possible to \emph{marginalize} over nuisance parameters. A
nuisance parameter is a component of the generative model that is of little
or physical interest but must be specified in order to generate the data.
Marginalization is the process of integrating over all possible values of
these parameters and hence propagating the effects of your uncertainty about
the true value to the final result. This operation is written as the integral
\begin{equation}
    \eqlabel{marginalization}
    p (\mathbf{X} | \boldsymbol{\Theta}) = \int
        p (\mathbf{X} | \boldsymbol{\Theta},\boldsymbol{\alpha}) \,
        p (\boldsymbol{\alpha}) \, \dd \boldsymbol{\alpha}
\end{equation}
where $p (\mathbf{X} | \boldsymbol{\Theta})$ is the marginalized likelihood
of the data $\mathbf{X}$ given the model parameters $\boldsymbol{\Theta}$
and $\boldsymbol{\alpha}$ is vector of nuisance parameters. In some specific
cases, the integral in \eq{marginalization} is analytically tractable but in
general, the likelihood function
$p (\mathbf{X} | \boldsymbol{\Theta},\boldsymbol{\alpha})$ is not a simple
integrable function. In fact, in many problems, the likelihood
function is actually the result of an extremely expensive numerical
simulation. In this regime, the integration must be calculated numerically
and it is often relatively efficient to approximate \eq{marginalization}
using MCMC sampling. If the likelihood function is expensive to
calculate, it is advantageous to use a sampling algorithm that reduces the
necessary number of likelihood evaluations. This also precludes the use of
second order methods (such as hybrid/Hamiltonian Monte Carlo) that require
the calculation of (numerical) gradients of the likelihood function.

Most uses of MCMC in the astrophysics literature are based on slight
modifications to the Metropolis-Hastings (M-H) method
\citep[e.g.][]{MacKay:2003}. Each step in a M-H chain is proposed using a
multivariate Gaussian centered on the current position of the chain. Since
each term in the covariance matrix of this proposal distribution is an
unspecified parameter, this method has $D\,[D+1]/2$ tuning parameters (where
$D$ is the dimension of the parameter space).  To make matters worse, the
performance of this sampler is very sensitive to the optimality of these
tuning parameters and there is no fool-proof method for choosing the values
correctly. As a result, many heuristic methods have been developed to attempt
to determine the optimal parameters in a data-driven way
\citep[e.g.][]{Gregory:2005,Dunkley:2005,Widrow:2008}. Unfortunately, these
methods all require ``burn-in'' phases where shorter Markov chains
are sampled and the results are used to tune the hyperparameters. This extra
cost is unacceptable when the likelihood calls are computationally heavy.

The problem with traditional sampling methods can be visualized by studying
the highly anisotropic density
\begin{equation}
    \eqlabel{anisotropic}
    p(\mathbf{x}) \propto \exp \left (-\frac{(x_1-x_2)^2}{2\,\epsilon}
                                        - \frac{(x_1+x_2)^2}{2} \right )
\end{equation}
which would be considered ``difficult'' by standard MCMC algorithms.
\Eq{anisotropic} can be transformed into the much easier problem of sampling
an isotropic Gaussian by an \emph{affine transformation} of the form
\begin{equation}
    y_1 = \frac{x_1-x_2}{\sqrt{\epsilon}} \, , \,\,\, y_2 = x_1 + x_2.
\end{equation}
Therefore, an algorithm that is \emph{affine invariant} will be insensitive to
covariances between parameters. An affine invariant algorithm is unaffected by
any transformation of the density of the form
\begin{equation}
    \mathbf{Y} = \mathbf{A}\, \mathbf{X} + \mathbf{b}.
\end{equation}

Extending earlier work by \citet{Christen:2007},
\citet[][hereafter GW]{Goodman:2010} proposed an affine invariant sampling
algorithm (\sect{algo}) with only two hyperparameters that can be tuned for
performance. \citet{Hou:2011} were the first group to implement this
algorithm to solve a physics problem. The implementation presented here is
an independent effort that has already proved effective in several projects
\citep[][Foreman-Mackey \& Widrow~2012, in prep.]{Lang:2011,
Bovy:2011, Dorman:2012}.
In what follows, we summarize the GW algorithm and the implementation
decisions made in \this. In \sect{parallel} we describe the small changes
that must be made to the algorithm to parallelize it. Finally, in \sect{api},
we outline the installation, usage and troubleshooting of the package.

\section{The Algorithm}\sectlabel{algo}

\subsection{The stretch move}

\citet{Goodman:2010} proposed an affine invariant ensemble sampling algorithm
informally called the ``stretch move''. For completeness and for clarity of
notation, we summarize the algorithm here and refer the interested reader to
the original paper for more details. This method involves simultaneously
evolving an ensemble of $K$ \emph{walkers}
$\mathbf{X} = \{ X_j, \forall j=1,\ldots,K \}$ where the proposal
distribution for one walker $k$ is based on the current positions of the
$K-1$ walkers in the \emph{complementary ensemble}
$\mathbf{X}_{[k]} = \{ X_j, \forall j \ne k \}$. In general, each $X_j$ is
also a vector in $n$ dimensions, where $n$ is the dimension of the parameter
space.

To update the position of a walker at position $\mathbf{X}_k$,
another walker $\mathbf{X}_j$ with $j \ne k$ is randomly chosen and then
a new position is proposed:
\begin{equation}
    \eqlabel{proposal}
    {X}_k (t) \to {Y} = {X}_j + Z \, [{X}_k (t) - {X}_j]
\end{equation}
where $Z$ is a random variable drawn from a distribution $g(z)$.  It is clear
that if $g(z)$ satisfies
\begin{equation}
    g(z^{-1}) = z \, g(z),
\end{equation}
the proposal of \eq{proposal} is symmetric. In this case, the chain will
satisfy detailed balance if the proposal is accepted with probability
\begin{equation}
    \eqlabel{acceptance}
    q = \min \left \{ 1, Z^{n-1} \,
                \frac{p(\mathbf{Y})}{p(\mathbf{X}_k(t))} \right \}
\end{equation}
where $n$ is the dimension of the parameter space. This procedure is then
repeated for each walker in the ensemble \emph{in series} following the
procedure shown in \algo{goodman}.

\citet{Goodman:2010} advocate for a particular form of $g(z)$, namely
\begin{equation}
    \eqlabel{goodmanprop}
    g(z) \propto \left \{ \begin{array}{ll}
        \displaystyle\frac{1}{\sqrt{z}} & \mathrm{if}\, z\in
                        \left [ \displaystyle\frac{1}{a}, a \right ], \\
        0 & \mathrm{otherwise}
    \end{array} \right .
\end{equation}
where $a$ is an adjustable scale parameter that \citet{Goodman:2010} set
to 2.

\begin{algorithm}
\caption{A single stretch move update step from \citet{Goodman:2010} where
    line \ref{line:hard} is generally the most computationally expensive
    step. \algolabel{goodman}}
\begin{algorithmic}[1]
\FOR{$k = 1, \ldots, K$}
    \STATE Draw a walker $X_j$ at random from the complementary ensemble %
        $\mathbf{X}_{[k]}(t)$
    \STATE $Z \gets z \sim g(z)$, \Eq{goodmanprop}
    \STATE $Y \gets X_j + Z \, [ X_k (t) - X_j]$
    \STATE $q \gets Z^{n-1} \, p(Y)/p(X_k(t))$ \label{line:hard}
    \STATE $R \gets r \sim [0, 1]$
    \IF{$R \ge q$, \eq{acceptance}}
        \STATE $X_k(t+1) \gets Y$
    \ELSE
        \STATE $X_k(t+1) \gets X_k(t)$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{The parallel stretch move}
\sectlabel{parallel}

It is tempting to na\"ively parallelize the stretch move algorithm by
simultaneously advancing each walker based on the state of the ensemble
instead of evolving the walkers in series. Unfortunately, this would no
longer satisfy detailed balance. Instead, if we split the ensemble into two
ensembles ($\mathbf{X}^{(0)}$ and $\mathbf{X}^{(1)}$) and
simultaneously update all the blue walkers --- using the stretch move
procedure --- based on the positions of \emph{only the red walkers} then the
outcome is a valid step for each of the walkers. Then, the red walkers are
advanced based only on the positions in the blue ensemble. The pseudocode for
this procedure is shown in \algo{parallel}. This code looks very similar to
that in \algo{goodman}, however, now the computationally expensive inner loop
(starting at line \ref{line:parallelloop} in \algo{parallel}) can be run in
parallel.

The performance of this method --- quantified by the autocorrelation time ---
is comparable to the traditional stretch move algorithm but the fact that one
can now take advantage of generic parallelization makes this generalization
extremely powerful.

\begin{algorithm}
\caption{The parallel stretch move update step
    \algolabel{parallel}}
\begin{algorithmic}[1]
\FOR{$i \in \{0, 1\}$}
    \FOR{$k = 1, \ldots, K/2$} \label{line:parallelloop}
        \STATE Draw a walker $X_j$ at random from the complementary ensemble %
            $\mathbf{X}^{(\sim i)} (t)$
        \STATE $Z \gets z \sim g(z)$, \Eq{goodmanprop}
        \STATE $Y \gets X_j + Z \, [ X^{(i)}_k (t) - X_j]$
        \STATE $q \gets Z^{n-1} \, p(Y)/p(X^{(i)}_k(t))$
        \STATE $R \gets r \sim [0, 1]$
        \IF{$R \ge q$, \eq{acceptance}}
            \STATE $X^{(i)}_k(t+\frac{1}{2}) \gets Y$
        \ELSE
            \STATE $X^{(i)}_k(t+\frac{1}{2}) \gets X_k(t)$
        \ENDIF
    \ENDFOR
    \STATE $t \gets t+\frac{1}{2}$
\ENDFOR

\end{algorithmic}
\end{algorithm}

\section{Benchmarks \& Tests}

\subsection{Measuring the performance}

A standard method of quantifying the performance of an MCMC sampler is to
estimate the autocorrelation time of the sampler on several densities.

The main goal of running a Markov chain is to measure the expectation value
(and variance) of a particular value (e.g.~$f$)
\begin{equation}
    \left < f(\mathbf{x}) \right > = \int f(\mathbf{x}) \, p (\mathbf{x}) \,
            \dd \mathbf{x}
\end{equation}
which can be approximated as
\begin{equation}
    \eqlabel{schainestim}
    \left < f(\mathbf{x}) \right > \approx \frac{1}{T_s} \sum_{t=1}^{T_s}
            f(\mathbf{X}(t))
\end{equation}
where $T$ is the length of the chain.  The generalization of \eq{schainestim}
to the case of the ensemble sampler is
\begin{equation}
    \eqlabel{echainestim}
    \left < f(\mathbf{x}) \right > \approx \frac{1}{T_e} \sum_{t=1}^{T_s}
        \left [ \frac{1}{K} \sum_{k = 1}^{K} f(\mathbf{X}_k(t)) \right ]
\end{equation}
where $K$ is the number of walkers.  The autocorrelation function of the chain
is then given by
\begin{equation}
    C (t) = \frac{1}{K^2} \lim_{t^\prime \to \infty} \mathrm{cov}
            \left [ \sum_{k = 1}^{K} f(\mathbf{X}_k (t+t^\prime)),
            \sum_{k = 1}^{K} f(\mathbf{X}_k (t^\prime)) \right ]
\end{equation}
and the integrated autocorrelation time is given by
\begin{equation}
    \tau = \sum_{t= -\infty} ^{\infty} \frac{C(t)}{C(0)} .
\end{equation}

\this~depends on the Python module
\project{acor}\footnote{\url{http://github.com/dfm/acor}} to estimate the
autocorrelation time. This module is a direct port of the original
algorithm \citep[described by][]{Goodman:2010} and implemented by those
authors in
C++.\footnote{\url{http://www.math.nyu.edu/faculty/goodman/software/acor}}

\subsection{Multivariate Gaussian distribution}

The simplest test of an MCMC sampler is its sampling performance on a highly
covariant multivariate Gaussian density. For the tests in the paper, we
randomly generated a 50 dimensional positive definite covariance tensor and
initial conditions for each walker. Then, each sampler was tested with the
same initial conditions for its performance on the density
\begin{equation}
    \pi (\mathbf{x}) \propto \exp\left ( -\frac{1}{2} \mathbf{x}^T \,
                                \Sigma^{-1} \, \mathbf{x} \right )
\end{equation}
for the same covariance tensor $\Sigma$ in each trial.

DFM: actually do some tests here...


\section{Usage}

\subsection{Installation}

The easiest way to install \this~is using
\pip\footnote{\url{http://pypi.python.org}}. Running the command
\begin{lstlisting}
% pip install emcee
\end{lstlisting}
at the command line of a UNIX-based system will install the package and its
\Python~dependencies. If you would like to install for all users, you might
need to run the above command with superuser permissions. \this~depends on
\Python~($>2.7$) and \numpy\footnote{\url{http://numpy.scipy.org}} ($>1.6$)
and the associated \texttt{dev} headers. On some systems, you might need to
install these packages separately. On \Ubuntu, you can install these
dependencies using the command:
\begin{lstlisting}
% apt-get python python-dev numpy numpy-dev
\end{lstlisting}

An alternative installation method is to download the source code from
\url{http://danfm.ca/emcee} and run
\begin{lstlisting}
% python setup.py install
\end{lstlisting}
in that directory. Make sure that you have \numpy~installed as well.

\subsection{Basic usage}
\begin{lstlisting}
import numpy as np
import emcee

def lnprobfn(x, mu, icov):
    diff = x-mu
    return -np.dot(diff,np.dot(icov,diff))/2.0

ndim  = 10
means = np.random.rand(ndim)
cov   = 0.5-np.random.rand(ndim**2).reshape((ndim, ndim))
cov   = np.triu(cov)
cov  += cov.T - np.diag(cov.diagonal())
cov   = np.dot(cov,cov)
icov  = np.linalg.inv(cov)

nwalkers = 100
p0 = [np.random.rand(ndim) for i in xrange(nwalkers)]

sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprobfn,
                        args=[means, icov])

pos,prob,state = sampler.run_mcmc(p0, None, 500)
sampler.clear_chain()

sampler.run_mcmc(pos, state, 2000)

print "Mean acceptance fraction:", np.mean(sampler.acceptance_fraction)
\end{lstlisting}

\subsection{API} \sectlabel{api}

\subsubsection{\code{\thisplain.ensemble}}

\class{EnsembleSampler}{*args, **kwargs}
    {Ensemble sampling following Goodman \& Weare (2010) with optional
    parallelization}

\begin{args}
    \param{k}{int}{The number of Goodman \& Weare ``walkers''.}
    \param{dim}{int}{Number of dimensions in the parameter space.}
    \param{lnpostfn}{callable}{%
        A function that takes a vector in the parameter space as input and
        returns the natural logarithm of the posterior probability for that
        position.
    }
\end{args}

\begin{kwargs}
    \param{a}{float}{The proposal scale parameter from \eq{goodmanprop}.
                     (default: \code{2.0})}
    \param{args}{list}{%
        Optional list of extra arguments for \code{lnpostfn}. \code{lnpostfn}
        will be called with the sequence \code{lnpostfn(p, *args)}.
    }
    \param{postargs}{list}{Alias of \code{args} for backwards compatibility.}
    \param{threads}{int}{The number of threads to use for parallelization.
        If \code{threads == 1}, then the \code{multiprocessing} is not used
        but if \code{threads > 1}, then a \code{Pool} object
        is created and calls to \code{lnpostfn} are run in parallel following
        \algo{parallel}. (default: \code{1})}
    \param{pool}{multiprocessing.Pool}{An alternative method of using the
        parallelized algorithm. If \code{pool is not None}, the value of
        \code{threads} is ignored and the provided \code{Pool} is used for
        all parallelization. (default: \code{None})}
\end{kwargs}

\begin{errors}
    \exception{AssertionError}{If \code{k < 2*dim} or if \code{k} is not even.}
\end{errors}

\begin{properties}
    \property{chain}{numpy.ndarray}{A pointer to the Markov chain itself. The
        shape of this array is \code{(k, dim, iterations/resample)}}
    \property{flatchain}{numpy.ndarray}{A shortcut for accessing \code{chain}
        flattened along the zeroth (walker) axis.}
    \property{lnprobability}{numpy.ndarray}{A pointer to the matrix of the
        value of \code{lnprobfn} produced at each step for each walker. The
        shape is \code{(k, iterations/resample)}.}
    \property{iterations}{int}{The number of steps that have been run in the
        chain.}
    \property{acceptance\_fraction}{numpy.ndarray}{An array (length: \code{k})
        of the fraction of steps accepted for each walker.}
    \property{acor}{numpy.ndarray}{The autocorrelation time of each parameter
        in the chain (length: \code{dim}) as estimated by the \code{acor}
        module.}
    \property{random\_state}{tuple}{The state of the internal random number
        generator. In practice, it's the result of calling \code{get\_state()}
        on a \code{numpy.random.mtrand.RandomState} object. You can try to
        set this property but be warned that if you do this and it fails, it
        will do so silently.}
\end{properties}

\begin{methods}
    \method{sample}{pos0, lnprob0=None, rstate0=None, iterations=1}
        {Advances the chain \code{iterations} steps as an iterator}{%
            \begin{args}
                \param{pos0}{numpy.ndarray}{
                    A list of the initial positions of the walkers in the
                    parameter space. The shape is \code{(k, dim)}.}
            \end{args}
            \begin{kwargs}
                \param{lnprob0}{numpy.ndarray}{
                    The list of log posterior probabilities for the walkers
                    at positions given by the \code{p0}. If \code{lnprob is
                    None}, the initial values are calculated. The shape is
                    \code{(k, dim)}.}
                \param{rstate0}{tuple}{The state of the random number
                    generator. \\
                    See \code{EnsembleSampler.random\_state} for details.}
                \param{iterations}{int}{The number of steps to run.
                    (default: \code{1})}
            \end{kwargs}
            \begin{yields}
                \param{pos}{numpy.ndarray}{
                    A list of the current positions of the walkers in the
                    parameter space. The shape is \code{(k, dim)}.}
                \param{lnprob}{numpy.ndarray}{
                    The list of log posterior probabilities for the walkers
                    at positions given by the \code{pos}. The shape is
                    \code{(k, dim)}.}
                \param{rstate}{tuple}{The state of the random number
                    generator.}
            \end{yields}
        }
    \method{run\_mcmc}{pos0, N, rstate0=None, lnprob0=None, **kwargs}
        {Iterate \code{sample} for \code{N} iterations and return the result.
        The parameters are passed directly to \code{sample} so see above for
        details.}{%
            \begin{returns}
                \param{pos}{numpy.ndarray}{
                    A list of the final positions of the walkers in the
                    parameter space. The shape is \code{(k, dim)}.}
                \param{lnprob}{numpy.ndarray}{
                    The list of log posterior probabilities for the walkers
                    at positions given by the \code{pos}. The shape is
                    \code{(k, dim)}.}
                \param{rstate}{tuple}{The final state of the random number
                    generator.}
            \end{returns}
        }
    \method{reset}{}{Clear \code{chain}, \code{lnprobability} and the
        bookkeeping parameters.}{}
    \method{clear\_chain}{}{An alias of \code{reset} kept for backwards
        compatibility.}{}

\end{methods}

\subsection{Issues \& Contributions}

The development of \this~is being coordinated on \github~at
\url{http://github.com/dfm/emcee} and contributions are welcome. If you
encounter any problems with the code, please report them at
\url{http://github.com/dfm/emcee/issues} (DFM: check this url) and consider
contributing a patch.


\begin{thebibliography}{}
\raggedright

\bibitem[Bovy~\etal~(2011)]{Bovy:2011}
 Bovy,~J., Rix,~H.-W., Liu,~C., Hogg,~D.~W., Beers,~T.~C., \& Lee,
Y.~S., 2011, \apj, submitted, arXiv:1111.1724 [astro-ph.GA]
% http://adsabs.harvard.edu/cgi-bin/bib_query?arXiv:1111.1724

\bibitem[Christen~(2007)]{Christen:2007}
{Christen}, J., \emph{A general purpose scale-independent MCMC algorithm},
technical report I-07-16, CIMAT, Guanajuato, 2007.

\bibitem[Dorman~\etal~(2012)]{Dorman:2012}
{Dorman},~C., {Guhathakurta},~P., {Fardal},~M.~A., {Geha},~M.~C.,
{Howley},~K.~M., {Kalirai},~J.~S., {Lang},~D., {Cuillandre}, J.,
{Dalcanton},~J., {Gilbert},~K.~M., {Seth},~A.~C., {Williams},~B.~F.,
\& {Yniguez},~B., 2012, \apj, submitted

\bibitem[Dunkley~\etal~(2005)]{Dunkley:2005}
{Dunkley}, J., {Bucher}, M., {Ferreira}, P.~G., {Moodley}, K., \& {Skordis}, C.,
2005, \mnras, 356, 925-936
% http://adsabs.harvard.edu/abs/2005MNRAS.356..925D

\bibitem[Goodman~\&~Weare~(2010)]{Goodman:2010}
Goodman,~J., \& Weare,~J.,
2010, Comm.\ App.\ Math.\ Comp.\ Sci., 5, 65

\bibitem[Gregory~(2005))]{Gregory:2005}
{Gregory}, P.~C., \emph{Bayesian Logical Data Analysis for the Physical Sciences},
Cambridge University Press, 2005
% http://adsabs.harvard.edu/abs/2005blda.book.....G

\bibitem[Hou~\etal~(2011))]{Hou:2011}
{Hou}, F., {Goodman}, J., {Hogg}, D.~W., {Weare}, J., \& {Schwab}, C.,
2011, arXiv:1104.2612
% http://adsabs.harvard.edu/abs/2011arXiv1104.2612H

\bibitem[Lang~\& Hogg~(2011))]{Lang:2011}
{Lang}, D. and {Hogg}, D.~W.,
2011, arXiv:1103.6038
% http://adsabs.harvard.edu/abs/2011arXiv1103.6038L

\bibitem[MacKay~(2003))]{MacKay:2003}
{MacKay}, D., \emph{Information Theory, Inference, and Learning Algorithms},
Cambridge University Press, 2003

\bibitem[Widrow~\etal~(2008))]{Widrow:2008}
{Widrow}, L.~M. and {Pym}, B. and {Dubinski}, J.,
2008, \apj, 679, 1239
% http://adsabs.harvard.edu/abs/2008ApJ...679.1239W

\end{thebibliography}

\end{document}


